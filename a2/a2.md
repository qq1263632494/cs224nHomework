## 使用交叉熵近似计算
$$CrossEntropy(y, \widehat{y})=-\sum_{w \in Vocab}y_{w}log(\widehat{y_{w}})$$&emsp;&emsp;其中$y$为one-hot编码的真实向量，$\widehat{y_{w}}$为预测值。
&emsp;&emsp;有$$CrossEntropy(y, \widehat{y})=J_{naive-softmax}=-log(P(O=o|C=c))$$
$$\frac{\partial J}{\partial \theta}=$$
$$\frac{\partial J}{\partial u_{o}} = [\sigma (u_{o}^{T}v_{c})-1]v_{c}$$
$$\frac{\partial J}{\partial u_{k}} = [1 - \sigma(-u_{k}^{T}v_{c})]v_{c}$$
$$J_{naive-softmax}=-log[\frac{e^{u_{o}^{T}v_{c}}}{\sum e^{u_{w}^{T}v_{c}}}]=-u_{o}^{T}v_{c}+log(\sum e^{u_{w}^{T}v_{c})$$